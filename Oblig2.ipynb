{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413e6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "nltk.download(\"brown\"); nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872275a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f549f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d515d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d7505",
   "metadata": {},
   "source": [
    "## 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48f2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data(dataset, train_test_validate):\n",
    "    if len(train_test_validate) == 3:\n",
    "        if sum(train_test_validate) == 1.:\n",
    "            news_test = []; news_dev_test = []; news_train = []\n",
    "            \n",
    "            dataset_len = len(dataset)\n",
    "            \n",
    "            slicer = list(int(ttv * dataset_len) for ttv in train_test_validate)\n",
    "            \n",
    "            news_test     = dataset[:slicer[0]]\n",
    "            news_dev_test = dataset[slicer[0]:slicer[0] + slicer[1]]\n",
    "            news_train    = dataset[slicer[0] + slicer[1]:]\n",
    "\n",
    "            return news_test, news_dev_test, news_train        \n",
    "        \n",
    "        else:\n",
    "            print(\"train_test_validate must be of length 3 with the sum of it's elements eqyal to 1\")\n",
    "            return \n",
    "    else:\n",
    "        print(\"train_test_validate must be of length 3 with the sum of it's elements eqyal to 1\")\n",
    "        return\n",
    "\n",
    "tagged_sents_uni = brown.tagged_sents(categories='news', tagset='universal')\n",
    "news_test, news_dev_test, news_train  = divide_data(tagged_sents_uni, [0.1,0.1, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde46bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623\n",
      "462 462 3699\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_sents_uni))\n",
    "print(len(news_test), len(news_dev_test), len(news_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf571e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8689\n"
     ]
    }
   ],
   "source": [
    "tagger_task_1a = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_task_1a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacf57a",
   "metadata": {},
   "source": [
    "## 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b8a56",
   "metadata": {},
   "source": [
    "I didn't quite manage this task but I think Unigram from chapter 5 of the nltk book needs to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052b7ad",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, \n",
    "                 features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87497f",
   "metadata": {},
   "source": [
    "## 1.2.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28532bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "tagger_task_2a = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(tagger_task_2a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ecb0a",
   "metadata": {},
   "source": [
    "Close to what we got in last exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a84a6",
   "metadata": {},
   "source": [
    "## 1.2.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1072c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1   0.857\n",
      "Alpha: 0.5   0.8749\n",
      "Alpha: 0.1   0.8695\n",
      "Alpha: 0.01   0.8683\n",
      "Alpha: 0.001   0.8651\n",
      "Alpha: 0.0001   0.8631\n"
     ]
    }
   ],
   "source": [
    "alpha =  [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "        \n",
    "for element in alpha:\n",
    "    clf = BernoulliNB(alpha=element)\n",
    "    tagger_task_2b = ScikitConsecutivePosTagger(news_train, features = pos_features, clf = clf)\n",
    "    print(\"Alpha:\", element, \" \", round(tagger_task_2b.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f8497",
   "metadata": {},
   "source": [
    "## 1.2.3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51838da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_pos_features(sentence, i, history):\n",
    "    \"\"\"\n",
    "    Improve pos_features to not only consider the previous word, but the word itself.\n",
    "    \"\"\"\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:],\n",
    "                \"suffix(4)\": sentence[i]}       # Word\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fcb17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1   0.8874\n",
      "Alpha: 0.5   0.9166\n",
      "Alpha: 0.1   0.9244\n",
      "Alpha: 0.01   0.9303\n",
      "Alpha: 0.001   0.933\n",
      "Alpha: 0.0001   0.934\n"
     ]
    }
   ],
   "source": [
    "for element in alpha:\n",
    "    clf = BernoulliNB(alpha=element)\n",
    "    tagger_task_2b = ScikitConsecutivePosTagger(news_train, features = improved_pos_features, clf = clf)\n",
    "    print(\"Alpha:\", element, \" \", round(tagger_task_2b.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6e6eccd",
   "metadata": {},
   "source": [
    "Did the extended feature selector beat the baseline? Intuitively, it should get as least as good\n",
    "accuracy as the baseline. Explain why!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e6787",
   "metadata": {},
   "source": [
    "## 1.3.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0c9901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter = 500)\n",
    "tagger_task_3a = ScikitConsecutivePosTagger(news_train, features = improved_pos_features, clf = clf)\n",
    "print(round(tagger_task_3a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19f99d",
   "metadata": {},
   "source": [
    "## 1.3.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3985e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01   0.8499\n",
      "Alpha: 0.1   0.9265\n",
      "Alpha: 1.0   0.9518\n",
      "Alpha: 10.0   0.9555\n",
      "Alpha: 100.0   0.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1000.0   0.95\n"
     ]
    }
   ],
   "source": [
    "new_alpha = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "for element in new_alpha:\n",
    "    clf = LogisticRegression(C = element, max_iter = 1000)\n",
    "    tagger_task_3b = ScikitConsecutivePosTagger(news_train, features = improved_pos_features, clf = clf)\n",
    "    print(\"Alpha:\", element, \" \", round(tagger_task_3b.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0a94856",
   "metadata": {},
   "source": [
    "C with best result\n",
    "C = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70ed6a",
   "metadata": {},
   "source": [
    "## 1.4.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581f83f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9663\n"
     ]
    }
   ],
   "source": [
    "def further_improved_pos_features(sentence, i, history):\n",
    "    \"\"\"\n",
    "    Improve pos_features to not only consider the previous word, but the word itself.\n",
    "    \"\"\"\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:],\n",
    "                \"suffix(4)\": sentence[i]}       # Word\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    if i < len(sentence) - 1:\n",
    "        features[\"next-word\"] = sentence[i+1]\n",
    "    else:\n",
    "        \"<END>\"\n",
    "    return features\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C = 10.0, max_iter = 1000)\n",
    "tagger_task_4a = ScikitConsecutivePosTagger(news_train, features = further_improved_pos_features, clf = clf)\n",
    "print(round(tagger_task_4a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be934066",
   "metadata": {},
   "source": [
    "## 1.4.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2e0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9663\n"
     ]
    }
   ],
   "source": [
    "def further_further_improved_pos_features(sentence, i, history):\n",
    "    \"\"\"\n",
    "    Improve pos_features to not only consider the previous word, but the word itself.\n",
    "    \"\"\"\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:],\n",
    "                \"suffix(4)\": sentence[i]}       # Word\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    \"\"\" # No improvments\n",
    "    if sentence[i][0].isupper():\n",
    "        features[\"word-first-upper\"] = 1\n",
    "    else:\n",
    "        0\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" # No improvments\n",
    "    if \"-\" in sentence[i]:\n",
    "        features[\"word-hypen\"] = 1\n",
    "    else:\n",
    "        0\n",
    "    \"\"\"\n",
    "        \n",
    "    if i < len(sentence) - 1:\n",
    "        features[\"next-word\"] = sentence[i+1]\n",
    "    else:\n",
    "        \"<END>\"\n",
    "        \n",
    "    return features\n",
    "\n",
    "clf = LogisticRegression(C = 10.0, max_iter = 1000)\n",
    "tagger_task_4b = ScikitConsecutivePosTagger(news_train, features = further_further_improved_pos_features, clf = clf)\n",
    "print(round(tagger_task_4b.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec25aee",
   "metadata": {},
   "source": [
    "## 1.5.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4be957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9659\n"
     ]
    }
   ],
   "source": [
    "# No improvments gained. Therefore will continue with previous tagger\n",
    "best_pos_features = further_improved_pos_features\n",
    "\n",
    "clf = LogisticRegression(C = 10.0, max_iter = 1000)\n",
    "tagger_task_5a = ScikitConsecutivePosTagger(news_train, features=best_pos_features, clf = clf)\n",
    "print(round(tagger_task_5a.evaluate(news_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb81334",
   "metadata": {},
   "source": [
    "Our result is a little bit worse than what we got on `news_dev_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da9aff",
   "metadata": {},
   "source": [
    "## 1.5.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "565d3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = [\"belles_lettres\", \"editorial\", \"fiction\", \"government\", \"humor\", \"learned\",\n",
    "       \"lore\", \"mystery\", \"religion\", \"reviews\", \"romance\", \"science_fiction\"]\n",
    "\n",
    "tagged_sents_5b = brown.tagged_sents(categories = all_sents, tagset = \"universal\")\n",
    "rest_test, rest_dev_rest, rest_train = divide_data(tagged_sents_5b, [0.1,0.1, 0.8])\n",
    "train = rest_train + news_train\n",
    "test = rest_test + news_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e690c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest same as 1b which I didn't quite manage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ea0c1",
   "metadata": {},
   "source": [
    "## 1.5.3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57b0875c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_732/4258205826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger_task_5dc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScikitConsecutivePosTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_pos_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_task_5cd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 10.0, max_iter = 1000)\n",
    "tagger_task_5dc = ScikitConsecutivePosTagger(train, features = best_pos_features, clf = clf)\n",
    "print(round(tagger_task_5cd.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcbea4",
   "metadata": {},
   "source": [
    "This didn't run on my computer as it drained my entire memory. But it should be correct. \n",
    "\n",
    "Comment on task: When I try to run 1.5.3b I get a \"memory limit reached\" problem and my kernel restarts. Unfortunately my laptop is quite old and cannot handle this task. However, I have implemented the subsequent tasks following. But I cannot test them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9404d",
   "metadata": {},
   "source": [
    "## 1.5.4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2251d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adventure = brown.tagged_sents(categories = 'adventure', tagset = 'universal')\n",
    "print(round(tagger_task_5cd.evaluate(adventure), 4))\n",
    "\n",
    "hobbies   = brown.tagged_sents(categories = 'hobbies',   tagset = 'universal')\n",
    "print(round(tagger_task_5cd.evaluate(hobbies), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d42a45",
   "metadata": {},
   "source": [
    "## 1.6.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_hmm_tagger_task_6a  = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "print(round(tagger_task_6a.evaluate(news_dev_test), 4))\n",
    "\n",
    "news_hmm_tagger2_task_6a = nltk.HiddenMarkovModelTagger.train(train)\n",
    "print(round(tagger2_task_6a.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e268745",
   "metadata": {},
   "source": [
    "## 1.6.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_tagger = nltk.PerceptronTagger(load = False)\n",
    "per_tagger.train(news_train)\n",
    "print(round(per_tagger.evaluate(news_dev_test), 4))\n",
    "\n",
    "per_tagger2 = nltk.PerceptronTagger(load = False)\n",
    "per_tagger2.train(train)\n",
    "print(round(per_tagger2.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc854d98",
   "metadata": {},
   "source": [
    "I can't compare the speed due to the reason I mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6064eb",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7af51",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfa7f8",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf56a15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of words: 400000\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim.downloader\n",
    "\n",
    "word_vec = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "words = list()\n",
    "for word in word_vec.vocab:\n",
    "    words.append(word)\n",
    "    \n",
    "print(\"Nr of words:\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffabb0",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0432ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(word_vec):\n",
    "    return np.sqrt(np.sum(word_vec * word_vec))\n",
    "\n",
    "# Formula https://en.wikipedia.org/wiki/Cosine_similarity under Definition\n",
    "def cos_between_vecs(word_vec1, word_vec2):\n",
    "    return np.dot(word_vec1, word_vec2) / (norm(word_vec1) * norm(word_vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8fa170",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9406e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with my func: 0.750769 \n",
      "Similarity with gensim func: 0.7507691\n"
     ]
    }
   ],
   "source": [
    "test1 = cos_between_vecs(word_vec[\"king\"], word_vec[\"queen\"])\n",
    "test2 = word_vec.similarity(\"king\", \"queen\")\n",
    "print(\"Similarity with my func:\", test1, \"\\nSimilarity with gensim func:\", test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947af04f",
   "metadata": {},
   "source": [
    "Very similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee11b2",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4876962",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24f2f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vehicle', 0.8630838394165039), ('truck', 0.8597878217697144), ('cars', 0.837166965007782), ('driver', 0.8185911178588867), ('driving', 0.7812635898590088)]\n",
      "[('suv', 0.8626778721809387), ('truck', 0.8475820422172546), ('vehicle', 0.8358194231987), ('jeep', 0.791013777256012), ('cars', 0.7910056114196777)]\n",
      "[('woman', 0.8183382749557495), ('girl', 0.7466667890548706), ('she', 0.695443332195282), ('her', 0.6720750331878662), ('mother', 0.6705917119979858)]\n",
      "[('woman', 0.8183382749557495), ('girl', 0.7466667890548706), ('she', 0.695443332195282), ('her', 0.6720750331878662), ('mother', 0.6705917119979858)]\n",
      "[('puppy', 0.6856015920639038), ('rottweiler', 0.5782471299171448), ('puppies', 0.5646027326583862), ('pug', 0.5142439007759094), ('toddler', 0.5109227895736694)]\n"
     ]
    }
   ],
   "source": [
    "wv = word_vec\n",
    "print(wv.most_similar('car', topn=5))\n",
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))\n",
    "\n",
    "# Oslo not in my model\n",
    "#print(wv.most_similar(positive=['Oslo', 'Sweden'], negative=['Norway'], topn=5))\n",
    "\n",
    "print(wv.most_similar(positive=['man', 'queen'], negative = ['king'], topn=5))\n",
    "print(wv.most_similar(positive=['queen', 'man'], negative=['king'], topn=5))\n",
    "\n",
    "print(wv.most_similar(positive=['kitten', 'dog'], negative=['cat'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b08ab8",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8127795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For queen the cosine was: 0.7834413\n",
      "For woman the cosine was: 0.55754864\n",
      "For man the cosine was: 0.39337876\n",
      "For king the cosine was: 0.85518366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('king', 0.8551837205886841),\n",
       " ('queen', 0.7834413647651672),\n",
       " ('monarch', 0.6933802366256714),\n",
       " ('throne', 0.6833109855651855),\n",
       " ('daughter', 0.680908203125),\n",
       " ('prince', 0.6713142395019531),\n",
       " ('princess', 0.664408266544342),\n",
       " ('mother', 0.6579325199127197),\n",
       " ('elizabeth', 0.6563301086425781),\n",
       " ('father', 0.6392419338226318)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = wv['king'] + wv['woman'] - wv['man']\n",
    "\n",
    "for noun in [\"queen\", \"woman\", \"man\", \"king\"]:\n",
    "    #cos_score = wv.similarity(a, noun) # Gives error?\n",
    "    cos_score = cos_between_vecs(a, wv[noun])\n",
    "    print(\"For\", noun, \"the cosine was:\", cos_score)\n",
    "\n",
    "\n",
    "wv.similar_by_vector(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038d64a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e2dd3",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1d9b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n",
      "tomatoes\n",
      "horse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work on my model\n",
    "#print(wv.doesnt_match(['Norway', 'Denmark', 'Finland', 'Sweden', 'Spain', 'Stockholm']))\n",
    "\n",
    "print(wv.doesnt_match([\"car\", \"bus\", \"train\", \"ship\", \"plane\", \"bike\"]))\n",
    "print(wv.doesnt_match([\"apple\", \"orange\", \"pear\", \"banana\", \"lemon\", \"tomatoes\"]))\n",
    "print(wv.doesnt_match([\"dog\", \"cat\", \"sheep\", \"cow\", \"pig\", \"horse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b5648",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0fc57",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b4ddad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between words in glove-wiki and brown: 26.36261780794833\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_brown = Word2Vec(sentences = brown.sents())\n",
    "\n",
    "words_in_brown = list()\n",
    "for word in model_brown.wv.vocab:\n",
    "    words_in_brown.append(word)\n",
    "    \n",
    "print(\"Difference between words in glove-wiki and brown:\", len(words) / len(words_in_brown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eaf8fb",
   "metadata": {},
   "source": [
    "Glove-Wiki is about 26 times larger than Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981738e",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4a418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('boy', 0.8149065971374512), ('woman', 0.7971924543380737), ('girl', 0.7894347906112671), ('young', 0.7244738936424255), ('old', 0.715161919593811)]\n",
      "[('boy', 0.8149065971374512), ('woman', 0.7971924543380737), ('girl', 0.7894347906112671), ('young', 0.7244738936424255), ('old', 0.715161919593811)]\n",
      "[('follow', 0.8970865607261658), ('choose', 0.8933405876159668), ('enrich', 0.8910219669342041), ('greeting', 0.8892170786857605), ('begin', 0.8875006437301636)]\n"
     ]
    }
   ],
   "source": [
    "print(model_brown.wv.most_similar(positive=['man', 'queen'], negative = ['king'], topn=5))\n",
    "print(model_brown.wv.most_similar(positive=['queen', 'man'], negative=['king'], topn=5))\n",
    "\n",
    "print(model_brown.wv.most_similar(positive=['kitten', 'dog'], negative=['cat'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017789b4",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce13989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6329672585445961\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "path=datapath('questions-words.txt')\n",
    "\n",
    "model_evaluated = wv.evaluate_word_analogies(path)\n",
    "print(model_evaluated[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
